---
title: "Using `axamodel` package "
author: "Etienne Kintzler"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc : true 

vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = F}
knitr::opts_chunk$set(cache = TRUE, message = FALSE)
```

This package aims at creating model for *Generalized Linear Models* (GLM). More specifically, it includes variable selection and interactions detection. The variable selection is performs with *Least Absolute Shrinkage and Selection Operator* (LASSO) applied to GLM. The interactions are detected with *Gradient Tree Boosting* (GTB) and Friedmanâ€™s *H-statistic*.

In this vignette the two principal functions of the package are presented : 

* the `lassoSelection`, used to select variables

* the `detectInteractions`, used to detect interactions and compute the resulting gain in GLM's performances. 

The model returned by `lassoSelection` will be called *base model*. The model that includes the variables selected by `lassoSelection` **and** the interactions found in `detectInteractions` will be called the *augmented model*.

## Installation

The package can be installed via git hub using :


```{r, eval = F}
devtools::install_git('https://github.com/etiennekintzler/automaticmodel.git')
```

for the my personnal repository or

```{r, eval = F}
devtools::install_git('https://github.axa.com/ds4a/axamodel.git')
```

for *Axa Github* repository.

Also, this package uses the development version of the package `gbm` which can be acquired using :

```{r, eval = F}
devtools::install_git('https://github.com/gbm-developers/gbm')
```




```{r, include = T, echo = F, results = 'hide', message = F}
library(axamodel)
library(dplyr)
```

### Loading the dataset

We will use the dataset *portfolio_example* of the package `axaml`

```{r, eval = T}
data <- axaml::portfolio_example
df   <- subset(x = data, select = -c(CalYear, Indtppd, Indtpbi, Numtppd, PolNum)) %>% 
  cleanData(n.levels = 10)
```

The `cleanData` function will clean the dataset removing variable with too many NA, too many levels, or too much correlation. Also, the resulting dataset will not contain any NA.  

The target variable is the number of claims `Numtpbi`, that takes its value in $\{0,1,2,4\}$. It will be thus modeled using poisson family.


## 1.  Using `lassoSelection` to select variables

### 1.1. Short introduction to Lasso-GLM

The `lassoSelection` returns the best linear model (i.e the best combination of variables supplied in that minimize a certain loss function) using LASSO-GLM procedure. Lasso is $L_1$-penalized least squares and performs variable selection through shrinking some estimates to zero. The program solved is the following :
\[
\min_\beta \left\lVert \mathbf{y}- \mathbf{X}\beta \right\rVert_2^2 + \left\lVert \beta\right\rVert_0
\]
The value of $\lambda$ is determined using k-fold cross-validation. The criterion used for the cross-validation is passed 
to `lassoSelection` function using the parameter `type.measure`.

### 1.2. Function call

The call for the `lassoSelection`is the following :

```{r, eval = T, echo = T, cache = T, results = "hide"}
best.glm <- lassoSelection(data           = df %>% dplyr::select(-Exposure), 
                           target         = 'Numtpbi',
                           family         = 'poisson', 
                           type.measure   = 'deviance',
                           lambda.type    = 'lambda.min',
                           offset         = log(df$Exposure),
                           nlambda        = 10,
                           nfold          = 5)
```


The function is based on `glmnet::cv.glmnet` for all the distributions but for gamma. For gamma distribution
`HDtweedie::cv.HDtweedie` is used. 

It is possible to add an offset to the function except for the gamma family (since `HDtweedie::cv.HDtweedie` does not enable it so far). If the offset is provided it is necessary to remove it from the object (a dataset) passed to the argument `data`.

### 1.3. Output diagnostic
The output of `lassoSelection` will be on object of class `LassoGLM` which elements can be accessed and plot via differents methods. Four methods will be presented here:

* `perf` which returns the performances (*Root Mean Squared Error* (RMSE) and *Gini index*) of the best model according to the Lasso-GLM procedure.

* `plotLambda` which plots the value of type.measure according to the different values of $\lambda$.

* `plot` which plots the Lorenz curve of the best model with the resulting *Gini Index*.

* `summary` provides a table with the selected variable and the non-selected variables.


```{r}
plotLambda(best.glm)
```

The value of lambda that optimize (minimize or maximize) `type.measure`, called `lambda.min`, is around $\exp(-6)$.
The lambda that gives the most regularized model such that its `type.measure` is within one standard error of the optimum's `type.measure`, called `lambda.1se`, is about $\exp(-4)$. For more details about these two types of lambda, please see `vignette('glmnet_beta', 'glmnet')`.


```{r}
perf(best.glm)
```

The following method plot the Lorenz Curve and the resulting Gini index.

```{r}
plot(best.glm)
```

The following command returns the variable selected by the procedure

```{r}
summary(best.glm)
```

One can also access the formula of this *base model* using :

```{r}
best.glm$formula
```

It formula can then plugged in `stats::glm` as in `stats::glm(formula = best.glm$formula, data = df, familu = poisson(link = 'log'))`.

## 2. Detecting interactions

The detection of interactions is performed by `detectInteractions`.

### 2.1. Short introduction to the detection of interactions using *H-statistic*

An interaction between two variables is a form of *non-additivity* of the response function. Let $F(x_1, x_2)$ the response function, there is *no* interaction between $x_1$ and $x_2$ if the response function writes :
$$F(x_1, x_2) = f_1(x_1)+ f_2(x_2) $$
where $f_i$ is a function that only depends on the variable $i$.

The *H-statistic* is a quantitty used within GBT framework to assess the presence of an interaction.
Considerong, two variables $x_1$ and $x_2$, the quantity writes:

\[
H_{1,2} = \frac{\sum(\widehat{F}_{1,2}(x_1, x_2)-\widehat{F}_1(x_1) - \widehat{F}_2(x_2)  )^2}{\sum(\widehat{F}_{1,2}(x_1, x_2))^2}
\]

Where $F_s, s\subset \{1,..,n\}$ is the partial dependence function (for more details on the *partial dependence function* and the *H-statistic* please see Friedman J.H. & Popescu, B. E. (2008). Predictive Learning via Rule Ensembles. *The Annals of Applied Statistics*, 916-954).

The interacted variables which *H-statistic* is superior to a **given** threshold will enter in the *augmented model*. This threshold will be called *selection threshold*.

### 2.2. Function call

```{r, eval = T, echo = T, cache = T, results = "hide"}
gbm.control <- list(distribution         = 'poisson', 
                    n.trees              = 3, 
                    interaction.depth    = 5, 
                    shrinkage            = .1, 
                    bag.fraction         = .5,
                    train.fraction       = .75, 
                    importance.threshold = 0,
                    max.select           = 20,
                    n.sample             = 1e3)

glm.control <- list(family           = poisson(link = 'log'), 
                    train.fraction   = .75, 
                    include          = F,
                    threshold.values = seq(0, 1, length.out = 20),
                    seed             = 1)

interactions <- detectInteractions(data             = df %>% dplyr::select(-Exposure), 
                                   target           = 'Numtpbi',
                                   offset           = df$Exposure,
                                   formula.best     = best.glm$formula,
                                   max.interactions = 30, 
                                   gbm.control      = gbm.control,
                                   glm.control      = glm.control)
```

The *base model* is specified in formula form in the parameter *formula.best*. The formula can either but the output of `lassoSelection` or can be user defined. For instance, if the user considers that the best model only contains the variable *Gender* and *Age*, he can pass it to the function as in : `formula.best = 'Numtpbi ~ Age + Gender'`.

This *base model* is compared with the *augmented model* that includes interactions(s).

The `gbm.control` argument takes the form of a list and contains the parameters necessary for fitting a Gradient Tree Boosting model using `gbm::gbm` such as `train.fraction`, `shrinkage`, `bag.fraction`, `n.trees` and `distribution` and `interaction.depth`.The other parameters in the list such as `importance.threshold`, `n.sample`, `max.select` are related to the computation of the *H-statistic*.

The `glm.control` argument also takes the form of a list and contains parameters used to fit a glm using `stats::glm`. The `threshold.values` is the *selection threshold* values for the *H-statistic* according to which the interactions are selected. There are as much *augmented models* as elements in `threshold.values`. The **best** `augmented model` is the one that maximise the performance criterion (*Gini index* or *Root Mean Squared Error* (RMSE)).

### 2.3. Output diagnostic

Four methods can be used to diagnostic the output of `detectInteractions` :

* `perf` compares the performances of the *base model* vs the **best** *augmented model*.

* `summary` give the interactions with the highest *H-statistic* value.

* `plotBest` plot the Lorenz curve of both the *base model* and the **best** *augmented model* along with the Gini gain.

* `plot` is the interaction plot of the i-th best interaction (the best is the one with the highest *H-statistic*).

* `returnBest` return the **best** *augmented model*.


```{r}
perf(interactions)
```

There is no substantial differences between the best *augmented model* and the *base model*.

```{r}
summary(interactions)
```

These are the interactions of the best *augmented model*.

```{r}
plot(interactions, simplify = 8)
```

This is the interaction plot for the best interaction found.

